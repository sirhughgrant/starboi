name: Sync stars from granolacowboy

on:
  schedule:
    - cron: "23 3 * * *"           # runs daily at 03:23 UTC; change if you like
  workflow_dispatch:
    inputs:
      dry_run:
        description: "Preview without starring"
        required: true
        default: "true"
      max_per_run:
        description: "Cap number to star per run"
        required: true
        default: "100"

jobs:
  sync:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: python -m pip install --upgrade pip requests

      - name: Run sync
        env:
          GITHUB_TOKEN: ${{ secrets.STAR_SYNC_TOKEN }}
          SOURCE_USER: "granolacowboy"
          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
          MAX_PER_RUN: ${{ github.event.inputs.max_per_run || '200' }}
        run: |
          python - <<'PY'
import os, time, sys, requests, random

TOKEN = os.environ["GITHUB_TOKEN"]
SOURCE_USER = os.environ.get("SOURCE_USER", "granolacowboy")
MAX_PER_RUN = int(os.environ.get("MAX_PER_RUN", "200"))
DRY_RUN = os.environ.get("DRY_RUN", "false").lower() == "true"

S = requests.Session()
S.headers.update({
    "Accept": "application/vnd.github+json",
    "Authorization": f"Bearer {TOKEN}",
    "X-GitHub-Api-Version": "2022-11-28",
    "User-Agent": "star-sync-action"
})

API = "https://api.github.com"

def paged(url, params=None):
    while url:
        r = S.get(url, params=params)
        handle_rate(r)
        r.raise_for_status()
        data = r.json()
        yield data
        url = None
        if "link" in r.headers:
            for part in r.headers["link"].split(","):
                if 'rel="next"' in part:
                    url = part[part.find("<")+1:part.find(">")]
                    break
        params = None

def handle_rate(r):
    # Obey secondary limits: if Retry-After is present, sleep.
    if r.status_code == 403 and "Retry-After" in r.headers:
        sleep_for = int(r.headers["Retry-After"])
        time.sleep(sleep_for + 1)

def list_starred(user):
    out = []
    url = f"{API}/users/{user}/starred"
    params = {"per_page": 100}
    for page in paged(url, params):
        out.extend([item["full_name"] for item in page])
    return set(out)

def list_my_starred():
    out = []
    url = f"{API}/user/starred"
    params = {"per_page": 100}
    for page in paged(url, params):
        out.extend([item["full_name"] for item in page])
    return set(out)

def star_repo(full_name):
    owner, repo = full_name.split("/", 1)
    url = f"{API}/user/starred/{owner}/{repo}"
    r = S.put(url, data=b"")   # per GitHub, empty body is fine
    if r.status_code in (204, 304):
        return True
    if r.status_code == 403 and "Retry-After" in r.headers:
        handle_rate(r)
        return star_repo(full_name)
    r.raise_for_status()
    return True

def gentle_backoff(i):
    # ~1 req/sec with slight jitter; grows a hair over time
    time.sleep(0.9 + random.random()*0.4 + min(i*0.01, 0.6))

def main():
    source = list_starred(SOURCE_USER)       # public stars of source user
    mine = list_my_starred()                 # your current stars
    missing = sorted(source - mine)
    if not missing:
        print("No new repos to star.")
        return
    batch = missing[:MAX_PER_RUN]
    print(f"Found {len(missing)} missing; processing {len(batch)} (MAX_PER_RUN={MAX_PER_RUN}). DRY_RUN={DRY_RUN}")

    done = 0
    for i, full in enumerate(batch, 1):
        if DRY_RUN:
            print(f"[dry-run] would star {full}")
        else:
            try:
                star_repo(full)
                done += 1
                if i % 25 == 0:
                    print(f"Progress: {i} done")
            except requests.HTTPError as e:
                print(f"Skip {full}: {e}", file=sys.stderr)
        gentle_backoff(i)

    print(f"Completed. Starred {done} repo(s).")

if __name__ == "__main__":
    main()
PY
